{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Urban Sound Dataset](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html) contains 8,732 labeled examples of length <= 4 seconds. Visually inspecting the different classes in the dataset aids in understanding the similarities and differences between the classes as well as any transformation we may need to apply to make the classification task easier for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "CLASSES = [\n",
    "    'air conditioner',\n",
    "    'car horn',\n",
    "    'children playing',\n",
    "    'dog bark',\n",
    "    'drilling',\n",
    "    'engine idling',\n",
    "    'gun shot',\n",
    "    'jackhammer',\n",
    "    'siren',\n",
    "    'street music'\n",
    "]\n",
    "    \n",
    "SOUND_FILE_PATHS = [\n",
    "    'Data/UrbanSound8K/audio/fold1/57320-0-0-7.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/24074-1-0-3.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/15564-2-0-1.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/31323-3-0-1.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/46669-4-0-35.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/89948-5-0-0.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/102305-6-0-0.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/103074-7-3-2.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/106905-8-0-0.wav',\n",
    "    'Data/UrbanSound8K/audio/fold1/108041-9-0-4.wav'\n",
    "]\n",
    "\n",
    "def load_sound_files(file_paths):\n",
    "    raw_sounds = []\n",
    "    for fp in file_paths:\n",
    "        samples, _ = librosa.load(fp)\n",
    "        raw_sounds.append(samples)\n",
    "    return raw_sounds\n",
    "    \n",
    "RAW_SOUNDS = load_sound_files(SOUND_FILE_PATHS)\n",
    "\n",
    "def plot_waves(sound_names, raw_sounds):\n",
    "    figure = plt.figure(figsize=(8, 16))\n",
    "    for idx, (name, sound) in enumerate(zip(sound_names, raw_sounds)):\n",
    "        plt.subplot(10, 1, idx + 1)\n",
    "        librosa.display.waveplot(np.array(sound), sr=22050)\n",
    "        plt.title(name)\n",
    "    figure.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_waves(CLASSES, RAW_SOUNDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the different classes we can make an important observations. The gun shot example is shorter in time than all the other examples. This observation is a problem because our models expect the input shapes or our training examples to be uniform.\n",
    "\n",
    "Also, perhaps we can use a better representation of our data to improve the accuracy of our model. In this case we will focus on spectrograms which include amplitude and frequency information over time. Something considered essential in most analyses of acoustic information.   \n",
    "\n",
    "Below we will define a function to pad the audio files with silence when they are too short or clip them when they are too long. Then, we will plot spectrograms of the different classes to see how our dataset is affected by the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "\n",
    "MAX_SECS = 4\n",
    "\n",
    "MAX_SAMPLES = SAMPLE_RATE * MAX_SECS\n",
    "\n",
    "def fix_raw_sounds(raw_sounds, length):\n",
    "    new_raw_sounds = []\n",
    "    for sound in raw_sounds:\n",
    "        if sound.shape[0] < length:\n",
    "            sound = np.pad(sound, (0, length - sound.shape[0]), 'constant')\n",
    "        elif sound.shape[0] > length:\n",
    "            sound = sound[:length]\n",
    "        new_raw_sounds.append(sound)\n",
    "    return new_raw_sounds\n",
    "\n",
    "RAW_SOUNDS = fix_raw_sounds(RAW_SOUNDS, MAX_SAMPLES)\n",
    "\n",
    "def plot_spectrograms(sound_names, raw_sounds):\n",
    "    figure = plt.figure(figsize=(8, 16))\n",
    "    for idx, (name, sound) in enumerate(zip(sound_names, raw_sounds)):\n",
    "        plt.subplot(10, 1, idx + 1)\n",
    "        plt.specgram(np.array(sound), cmap='jet', Fs=22050)\n",
    "        plt.colorbar()\n",
    "        plt.title(name)\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Frequency (hz)\")\n",
    "    figure.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_spectrograms(CLASSES, RAW_SOUNDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fixed our data we want to start working on the input pipeline. We also want to track some useful statistics in the process and save our dataset in a more suitable format for ingestion by TensorFlow models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
